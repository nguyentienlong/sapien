// Copyright 2018
//
// Author: mail2ngoclinh@gmail.com liblinear

#include <cmath>
#include <cstring>
#include <memory>

#include "sapien/solver/trust_region_minimizer.h"
#include "sapien/internal/sapien_math.h"
#include "glog/logging.h"

namespace sapien {

using internal::sapien_dot;
using internal::sapien_axpy;
using internal::sapien_xDiagy;
using internal::sapien_set;
using internal::sapien_scal;


TrustRegionMinimizer::TrustRegionMinimizer()
    : options_(TrustRegionMinimizer::Options()) {}

TrustRegionMinimizer::TrustRegionMinimizer(const Options& options)
    : options_(options) {}

void TrustRegionMinimizer::Minimize(const SecondOrderFunction* obj_function,
                                    double* solution) {
  CHECK_NOTNULL(obj_function);
  CHECK_NOTNULL(solution);

  // TODO(Linh): CHECK options

  const size_t n = obj_function->n_variables();

  // Init gradient
  std::unique_ptr<double[]> gradient(new double[n]);
  obj_function->Gradient(solution, gradient.get());
  double gradient_norm2 = sapien_dot(n, gradient.get(), gradient.get());

  // Keep track of values of obj_function
  double function_value, new_function_value;
  function_value = (*obj_function)(solution);

  double actual_reduction, predicted_reduction;

  // The step generated by conjugate gradient subproblem
  std::unique_ptr<double[]> trust_region_step(new double[n]);

  // The approximation of the Hessian matrix.
  // See [1], section 5 for more details.
  std::unique_ptr<double[]> approximate_hessian(new double[n]);

  // Since we solve the subproblem using preconditioned nonlinear conjugate
  // gradient, we have:
  //
  //  preconditioned_trust_region_step = Preconditioner^T * trust_region_step
  //
  // L2 norm of preconditioned_trust_region_step is:
  //
  //  trust_region_step^T *
  //  (Preconditioner * Preconditioner^T) * trust_region_step
  //
  // Given that approximate_hessian = Preconditioner * Preconditioner^T,
  // we have L2 norm of preconditioned_trust_region_step is:
  //
  //  trust_region_step^T * (approximate_hessian) * trust_region_step
  double preconditioned_trust_region_step_norm;
  double preconditioned_trust_region_step_norm_scale;

  double gradient_dot_trust_region_step;

  // The residual of the subproblem.
  //
  // The subproblem using preconditioned nonlinear conjugate gradient
  // to estimate the minimizer of this function:
  //
  //  q(s) = f(x_k) + f'(x_k) * s + 1/2 * s^T * f''(x_k) * s
  //  w.r.t |s| <= trust_region_radius
  //
  // The residual is simply -q'(s) = -(f'(x_k) + f''(x_k) * s)
  std::unique_ptr<double[]> subproblem_residual(new double[n]);

  double trust_region_radius;
  bool reach_trust_region_boundary;

  // new_solution = solution + trust_region_step
  std::unique_ptr<double> new_solution(new double[n]);

  const double epsilon = options().tolerance * options().tolerance *
      gradient_norm2;
  size_t iter = 0;

  while (iter < options().max_num_iterations && gradient_norm2 > epsilon) {
    // Approximate the Hessian matrix (see [1], section 5 for more details)
    //
    //  approximate_hessian = alpha * hessian_diag + (1 - alpha)
    //
    // Here we chose alpha to be 0.01
    obj_function->HessianDiag(solution, approximate_hessian.get());
    for (size_t i = 0; i < n; ++i) {
      approximate_hessian[i] = 0.99 + 0.01 * approximate_hessian[i];
    }

    // Init trust_region_radius = gradient^T * approximate_hessian * gradient
    // Note that the approximate_hessian is a diagonal matrix (which is
    // the diagonal of the true Hessian matrix).
    if (iter == 0) {
      trust_region_radius = 1e4;
      //     sapien_xDiagy(n, gradient.get(), approximate_hessian.get(),
      //                   gradient.get());
      // trust_region_radius = std::sqrt(trust_region_radius);
    }

    // Solve subproblem using preconditioned nonlinear conjugate gradient
    //
    //  q(s) = f'(x_k) * s + 1/2 s^T * f''(x_k) * s
    //  w.r.t |s| <= trust_region_radius
    //
    // The preconditioner chosen is the diagonal of the true Hessian matrix
    // Note that we drop the f(x_k) term since it is constant anyway.
    SolveSubproblem(obj_function, gradient.get(),
                    approximate_hessian.get(),
                    trust_region_radius,
                    trust_region_step.get(),
                    subproblem_residual.get(),
                    &reach_trust_region_boundary);

    LOG(INFO) << "trust_region_step: " << trust_region_step[0] << ", "
              << trust_region_step[1];
    // Update new_solution
    //
    //  new_solution = solution + trust_region_step
    std::memcpy(new_solution.get(), trust_region_step.get(),
                n * sizeof(double));
    sapien_axpy(n, 1.0, solution, new_solution.get());

    gradient_dot_trust_region_step = sapien_dot(n, gradient.get(),
                                                trust_region_step.get());
    new_function_value = (*obj_function)(new_solution.get());
    actual_reduction = function_value - new_function_value;

    // predicted_reduction = -q(s)
    //  = - (g^T * s + 0.5 * s^T * H * s)
    //  = 0.5 * s^T * (-g - H * s) - 0.5 * s^T * g
    //  = 0.5 * s^T * subproblem_residual - 0.5 * s^T * g
    //  = 0.5 * (s^T * subproblem_residual - g^T * s)
    //
    // (g, H is the gradient and (approximate) Hessian matrix of the
    //  obj_function at current iterate, respectively)
    // Note that, this is the negate of the actual predicted reduction but
    // it doesn't matter since what wee really need is the ratio between
    // the actual reduction and the predicted reduction.
    predicted_reduction = 0.5 * (
        sapien_dot(n, trust_region_step.get(), subproblem_residual.get()) -
        gradient_dot_trust_region_step);

    // We have:
    //
    //  preconditioned_trust_region_step
    //    = Preconditioner^T * trust_region_step
    //    = trust_region_step^T * approximate_hessian * trust_region_step
    //
    // using the fact that
    //  approximate_hessian = Preconditioner * Preconditioner^T
    preconditioned_trust_region_step_norm =
        sapien_xDiagy(n, trust_region_step.get(),
                      approximate_hessian.get(),
                      trust_region_step.get());
    preconditioned_trust_region_step_norm =
        std::sqrt(preconditioned_trust_region_step_norm);

    // On the first iteration, adjust the initial trust_region_radius
    if (iter == 0) {
      trust_region_radius =
          std::min(trust_region_radius,
                   preconditioned_trust_region_step_norm);
    }

    // Compute the preconditioned trust region step scaler
    double tmp1 = (new_function_value - function_value -
                  gradient_dot_trust_region_step);
    if (tmp1 <= 0) {
      preconditioned_trust_region_step_norm_scale = options().sigma3;
    } else {
      preconditioned_trust_region_step_norm_scale =
          std::max(options().sigma1,
                   -0.5 * (gradient_dot_trust_region_step / tmp1));
    }

    // Update trust_region_radius.
    // TODO(Linh): Consider to factor out these shit!
    double tmp2 =
        preconditioned_trust_region_step_norm_scale *
        preconditioned_trust_region_step_norm;
    if (actual_reduction < options().eta0 * predicted_reduction) {
      trust_region_radius =
          std::min(tmp2, options().sigma2 * trust_region_radius);
    } else if (actual_reduction < options().eta1 * predicted_reduction) {
      trust_region_radius =
          std::max(options().sigma1 * trust_region_radius,
                   std::min(tmp2, options().sigma2 * trust_region_radius));
    } else if (actual_reduction < options().eta2 * predicted_reduction) {
      trust_region_radius =
          std::max(options().sigma1 * trust_region_radius,
                   std::min(tmp2, options().sigma3 * trust_region_radius));
    } else if (reach_trust_region_boundary) {
        trust_region_radius = options().sigma3 * trust_region_radius;
    } else {
        trust_region_radius =
            std::max(trust_region_radius,
                     std::min(tmp2, options().sigma3 * trust_region_radius));
    }

    // Update solution
    if (actual_reduction > options().eta0 * predicted_reduction) {
      // accepted the step
      ++iter;
      std::memcpy(solution, new_solution.get(), n * sizeof(double));
      function_value = new_function_value;
      obj_function->Gradient(solution, gradient.get());
      gradient_norm2 = sapien_dot(n, gradient.get(), gradient.get());
    }

    // CHECK Numeric overflow

    if (function_value < -1.0e+32) {
      LOG(WARNING) << "function_value < -1.0e+32";
      break;
    }

    if (predicted_reduction <= 0) {
      LOG(WARNING) << "predicted_reduction <= 0";
      // break;
    }

    if (std::fabs(actual_reduction) <=
        1.0e-12 * std::fabs(function_value) &&
        std::fabs(predicted_reduction) <=
        1.0e-12 * std::fabs(function_value)) {
      LOG(WARNING) << "actual and predicted reductions are too small";
      break;
    }
  }

  // Check for convergence
  if (iter >= options().max_num_iterations && gradient_norm2 > epsilon) {
    LOG(WARNING) << "Exceeded max_num_iterations before convergence";
  }
}

void TrustRegionMinimizer::
SolveSubproblem(const SecondOrderFunction* obj_function,
                const double* gradient,
                const double* approximate_hessian,
                const double trust_region_radius,
                double* trust_region_step,
                double* residual,
                bool* reach_trust_region_boundary) const {
  const size_t n = obj_function->n_variables();

  // Init solution
  sapien_set(n, 0.0, trust_region_step);

  // Init residual
  for (size_t i = 0; i < n; ++i) {
    residual[i] = -gradient[i];
  }

  // Dot product between inverse Hessian matrix and residual vector
  std::unique_ptr<double[]> M_inv_r(new double[n]);
  for (size_t i = 0; i < n; ++i) {
    M_inv_r[i] = residual[i] / approximate_hessian[i];
  }

  // Init search direction
  // We have:
  //
  //  preconditioned_search_direction = preconditioned_initial_residual
  //
  // And also:
  //
  //  preconditioned_search_direction
  //   = Preconditioner^T * search_direction
  //
  //  preconditioned_initial_residual
  //   = Preconditioner^{-1} * residual
  //
  //  Precondtioner * Precondtioner^T = M (approximate of Hessian)
  //
  // So that:
  //
  //  Preconditioner^T * search_direction = Preconditioner^{-1} * residual
  //  =>
  //  search_direction = (P^T)^-1 * P^-1 * r0
  //                   = (P * P^T)^-1 * r0
  //                   = M^-1 * r0
  //
  //  P, r0, M is the Preconditioner, residual, and Hessian matrix,
  //  respectively.
  std::unique_ptr<double[]> search_direction(new double[n]);
  std::memcpy(search_direction.get(), M_inv_r.get(), n * sizeof(double));

  // Squared norm of precondtioned residual
  // We have:
  //
  //  preconditioned_residual = Precondtioner^{-1} * residual
  //
  // So that:
  //
  //  preconditioned_residual_norm2
  //   = preconditioned_residual^T * preconditioned_residual
  //   = r^T * (P^-1)^T * P^-1 * r
  //   = r^T * (P * P^T)^-1 * r
  //   = r^T * M^-1 * r
  //
  //  r, P, M is residual, Preconditioner, and Hessian matrix, respectively
  double preconditioned_residual_norm2 =
      sapien_dot(n, residual, M_inv_r.get());
  double previous_preconditioned_residual_norm2;

  // Dot product between the exact Hessian matrix and current search
  // direction
  std::unique_ptr<double[]> Hd(new double[n]);

  double step_size;
  double beta;
  double trust_region_radius_squares =
      trust_region_radius * trust_region_radius;
  double preconditioned_trust_region_step_norm2;

  const double tolerance = options().subproblem_tolerance *
      options().subproblem_tolerance * preconditioned_residual_norm2;

  while (preconditioned_residual_norm2 > tolerance) {
    // Compute step_size
    obj_function->HessianDot(search_direction.get(), Hd.get());
    step_size = preconditioned_residual_norm2 /
        sapien_dot(n, search_direction.get(), Hd.get());

    // Update trust region step
    sapien_axpy(n, step_size, search_direction.get(), trust_region_step);

    // Compute the preconditioned_trust_region_step_norm to see if
    // we are still within or outside the ball.
    // We have:
    //
    //  s_hat = P^T * s
    //  => s_hat_norm2 = s_hat^T * s_hat
    //                 = s^T * P * P^T * s
    //                 = s^T * (Hessian) * s
    //  s_hat, s, P is the preconditioned trust region step, the trust region
    //  step, the preconditioner matrix, respectively.
    // Note that we only use the approximate of the Hessian matrix which
    // is the diagonal of the true Hessian approximate_hessian.
    preconditioned_trust_region_step_norm2 =
        sapien_xDiagy(n, trust_region_step, approximate_hessian,
                      trust_region_step);

    if (preconditioned_trust_region_step_norm2 >
        trust_region_radius_squares) {
      // We're outside the ball.
      // We'll find tau such that:
      //
      //  L2_norm(s_hat + tau * d_hat) = trust_region_radius
      //
      // And output:
      //
      //  s = (P^{-1})^T * new_s_hat
      //    = (P^{-1})^T * (s_hat + tau * d_hat)
      //    = s + tau * d
      //
      //  (s_hat = P^T * s; d_hat = P^T * s)
      //  s, s_hat, d, d_hat, P is the trust region step, the preconditioned
      //  trust region step, the search direction, the precondtioned search
      //  direction, and the preconditioner matrix, respectively.

      // Undo the last step
      sapien_axpy(n, -step_size, search_direction.get(),
                  trust_region_step);

      // Solve one variable quadratic equation.
      // We have:
      //
      //  l2_norm(s_hat + tau * d_hat) = trust_region_radius
      //
      //  (s_hat + tau * d_hat)^T * (s_hat + tau * d_hat)
      //    = trust_region_radius_squares
      //
      //  (P^T * s + tau * P^T * d)^T * (P^T * s + tau * P^T * d)
      //    = trust_region_radius_squares
      //
      //  (d^T*M*d) * tau^2 + 2*(s^T*M*d) * tau + s^T*M*s
      //    = trust_region_radius_squares
      //
      // Let a = d^T*M*d, b = s^T*M*d,
      //     c = s^T*M*s - trust_region_radius_squares,
      // we have:
      //
      //  a * tau^2 + 2b * tau + c = 0
      //
      // Since s^T*M*s = s_hat_norm2, and we're currently inside the ball
      // (we have undone the last step) we have c < 0. If M is SPD (
      //  symmetric positive definite) we also have a > 0. So the
      // single variable quadratic function above has two roots:
      //
      //  tau1 = (-b - rad) / a ( < 0 )
      //  tau2 = (-b + rad) / a ( > 0 )
      //  in which rad = sqrt(b^2 - a*c)
      //
      // Since we only take positive step, tau2 is our desired solution
      double a = sapien_xDiagy(n, search_direction.get(),
                               approximate_hessian,
                               search_direction.get());
      double b = sapien_xDiagy(n, trust_region_step,
                               approximate_hessian,
                               search_direction.get());
      double c = sapien_xDiagy(n, trust_region_step,
                               approximate_hessian,
                               trust_region_step) -
          trust_region_radius_squares;
      double rad = std::sqrt(b * b - a * c);

      step_size = (rad - b) / a;
      sapien_axpy(n, step_size, search_direction.get(), trust_region_step);
      sapien_axpy(n, -step_size, Hd.get(), residual);
      *reach_trust_region_boundary = true;
      break;
    }

    // Normal preconditioned conjugate gradient updates

    sapien_axpy(n, -step_size, Hd.get(), residual);

    for (size_t i = 0; i < n; ++i) {
      M_inv_r[i] = residual[i] / approximate_hessian[i];
    }

    previous_preconditioned_residual_norm2 = preconditioned_residual_norm2;
    preconditioned_residual_norm2 =
        sapien_dot(n, residual, M_inv_r.get());
    beta = preconditioned_residual_norm2 /
        previous_preconditioned_residual_norm2;
    sapien_scal(n, beta, search_direction.get());
    sapien_axpy(n, 1.0, M_inv_r.get(), search_direction.get());
  }
}

}  // namespace sapien
